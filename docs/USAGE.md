# TULeCZech

**TULeCZech** je embedovac√≠ jazykov√Ω model urƒçen√Ω pro ƒçesk√Ω jazyk. Je navr≈æen pro √∫lohy jako s√©mantick√© vyhled√°v√°n√≠, vyhled√°v√°n√≠ podle podobnosti (similarity search), nebo jako souƒç√°st syst√©m≈Ø Retrieval-Augmented Generation (RAG).

Model je postaven na architektu≈ôe **xlm-RoBERTa-base** a doladƒõn (fine-tuned) v√Ωhradnƒõ na ƒçesk√Ωch datech.

üìÑ Pro detailn√≠ informace o tr√©nov√°n√≠, architektu≈ôe a pou≈æit√Ωch datech kliknƒõte [zde](./MODEL.md).

---

## üß† Z√≠sk√°n√≠ modelu

Model je dostupn√Ω na [HuggingFace Hubu](https://huggingface.co/tauchmand/tuleczech) a lze ho vyu≈æ√≠t nƒõkolika zp≈Øsoby:

- **Manu√°ln√≠ sta≈æen√≠** ze z√°lo≈æky `Files and versions`
- **Automatick√© naƒçten√≠ pomoc√≠ knihoven** `transformers` nebo `sentence-transformers` *(doporuƒçeno)*

---

## üöÄ Rychl√Ω start: pou≈æit√≠ s `transformers`

```python
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel

def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)
    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]

input_texts = [
    "query: Kolik b√≠lkovin by mƒõla ≈æena j√≠st",
    "query: ÂçóÁìúÁöÑÂÆ∂Â∏∏ÂÅöÊ≥ï",
    "passage: Obecnƒõ plat√≠, ≈æe pr≈Ømƒõrn√Ω po≈æadavek na p≈ô√≠jem b√≠lkovin ...",
    "passage: 1.Ê∏ÖÁÇíÂçóÁìú‰∏ù ÂéüÊñô:Â´©ÂçóÁìúÂçä‰∏™ ..."
]

tokenizer = AutoTokenizer.from_pretrained("tauchmand/tuleczech")
model = AutoModel.from_pretrained("tauchmand/tuleczech")

batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors="pt")
outputs = model(**batch_dict)
embeddings = average_pool(outputs.last_hidden_state, batch_dict["attention_mask"])
embeddings = F.normalize(embeddings, p=2, dim=1)

scores = (embeddings[:2] @ embeddings[2:].T) * 100
print(scores.tolist())
```

---

## ‚ú® Alternativa: pou≈æit√≠ se `sentence-transformers`

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("tauchmand/tuleczech")

input_texts = [
    "Kolik b√≠lkovin by mƒõla ≈æena j√≠st",
    "ÂçóÁìúÁöÑÂÆ∂Â∏∏ÂÅöÊ≥ï",
    "Obecnƒõ plat√≠, ≈æe pr≈Ømƒõrn√Ω po≈æadavek na p≈ô√≠jem b√≠lkovin ...",
    "1.Ê∏ÖÁÇíÂçóÁìú‰∏ù ÂéüÊñô:Â´©ÂçóÁìúÂçä‰∏™ ..."
]

embeddings = model.encode(input_texts, normalize_embeddings=True)
```

> üì¶ Po≈æadovan√© z√°vislosti:  
> ```bash
> pip install sentence_transformers~=2.2.2
> ```

---

## üåç Jazykov√° podpora

Model je zalo≈æen na **xlm-RoBERTa-base**, kter√Ω podporuje v√≠ce ne≈æ 100 jazyk≈Ø. Vzhledem k doladƒõn√≠ v√Ωhradnƒõ na ƒçesk√Ωch datech je **optimalizov√°n pouze pro ƒçe≈°tinu**.

---

## üìä V√Ωsledky na SQuAD

| **model**                                | **z√°kladn√≠ model**       | **acc@1 [%]** | **acc@3 [%]** | **acc@10 [%]** |
|------------------------------------------|---------------------------|---------------|---------------|----------------|
| **TULeCZech (n√°≈° model)**                | **xlm-RoBERTa-base**      | **63.7**      | **83.6**      | **92.1**       |
| Multilingual-E5-base                     | xlm-RoBERTa-base          | 65.4          | 86.8          | 91.7           |
| Multilingual-E5-small                    | xlm-RoBERTa-small         | 63.0          | 85.0          | 90.5           |
| distiluse-base-multilingual-cased-v2     | BERT                      | 42.6          | 68.5          | 77.3           |
| LaBSE                                    | BERT                      | 42.3          | 68.2          | 76.9           |
| Seznam/simcse-dist-mpnet-czeng-cs-en     | BERT                      | 36.1          | 62.1          | 71.8           |
| Seznam/RetroMAE-small-cs                 | BERT                      | 27.0          | 49.9          | 59.9           |
| Seznam/simcse-small-e-czech              | ELECTRA                   | 3.3           | 9.9           | 14.8           |
---

## ü¶ö Tr√©novac√≠ datasety

| **Dataset**     | **Poƒçet dvojic [tis.]** |
|-----------------|--------------------------|
| DaReCzech       | 97                       |
| SQuAD v1.1 (cz) | 18.5                     |
| SQuAD v2.0 (cz) | 18.6                     |
| iDnes           | 500                      |
| **Celkem**      | **634.3**                |


- Pro dal≈°√≠ podrobnosti p≈ôejdƒõte na [MODEL.md](./MODEL.md)

---

## ‚ö†Ô∏è Limitace

- Dlouh√© vstupy jsou o≈ô√≠znuty na **maxim√°lnƒõ 512 token≈Ø**
- Model je optimalizovan√Ω pouze pro ƒçe≈°tinu

---

## ‚ùì FAQ

**1. Mus√≠m pou≈æ√≠vat prefixy `query:` a `passage:`?**  
Ne, model funguje i bez nich. Prefixy ale mohou m√≠rnƒõ zlep≈°it v√Ωkon ve specifick√Ωch √∫loh√°ch.

**2. Proƒç je sk√≥re podobnosti vƒõt≈°inou mezi 0.3‚Äì0.5?**  
Je to d√°no pou≈æitou ztr√°tovou funkc√≠ (MultipleNegativesRankingLoss). Absolutn√≠ hodnota sk√≥re nen√≠ d≈Øle≈æit√° ‚Äì kl√≠ƒçov√© je **po≈ôad√≠ v√Ωsledk≈Ø**.

---

## üìö Citace

Pokud tento model nebo projekt vyu≈æ√≠v√°te, zva≈æte pros√≠m uveden√≠ n√°sleduj√≠c√≠ citace:

```bibtex
@article{tulezech,
    title={Tvorba jazykov√©ho modelu},
    author={Tauchman Denis, Martin Pol√°ƒçek},
    journal={...},
    year={2025}
}
